Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100),freq=12), order=c(1,0,0), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100),freq=12), order=c(0,0,1), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100),freq=12), order=c(0,0,0), seasonal=c(0,0,1))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100),freq=12), order=c(1,0,0), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100),freq=12), order=c(0,0,1), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100)+25,freq=12), order=c(0,0,1), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100,0,0.01),freq=12), order=c(0,0,1), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100,0,0.01),freq=12), order=c(1,0,0), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
model <- Arima(ts(rnorm(100,1,0.01),freq=12), order=c(1,0,0), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100,1,0.0001),freq=12), order=c(1,0,0), seasonal=c(0,0,0))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
Pacf(foo)$acf
model <- Arima(ts(rnorm(100,1,0.0001),freq=12), order=c(0,0,0), seasonal=c(0,0,1))
foo <- simulate(model, nsim=200)
Acf(foo)$acf
thepacf=ARMAacf (ar = c(.6,0,0,0,0,0,0,0,0,0,0,.5,-.30),lag.max=30,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(0,0,0,0,0,0,0,0,0,0,0,.5),lag.max=30,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(0,0,0,0,0,0,0,0,0,0,0,.5),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(0,0,0,0,0,0,0,0,0,0,0,.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(0,0,0,0,0,0,0,0,0,0,0,-0.5),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf
thepacf=ARMAacf (ar = c(0,0,0,0,0,0,0,0,0,0,0,0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ar = c(1,0,0,0,0,0,0,0,0,0,0,0),lag.max=90,pacf=T)
thepacf=ARMAacf (ar = c(1),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ar = c(1),lag.max=90,acf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(1),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(-1),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ar = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = -0.9,lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ar = -0.9,lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (aq = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ar = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ar = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,acf=T)
thepacf=ARMAacf (ar = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=F)
plot (thepacf,type="h")
thepacf=ARMAacf (ar = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
thepacf=ARMAacf (ma = c(0,0,0,0,0,0,0,0,0,0,0,-0.9),lag.max=90,pacf=T)
plot (thepacf,type="h")
library(tm)
library(SnowballC)
library(dplyr)
pres_stats <- data.frame(
row.names = c("Democrat", "Barrack Obama", "William Clinton", "Republican", "George W. Bush", "Donald Trump"),
Words = c(53165, 26502, 26663, 53080, 26063, 27017))
knitr::kable(pres_stats,booktabs = T)
clean_corp <- function(corp) {
corp %>%
tm_map(tolower) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, c(stopwords("english"), "president", "applause", "will"))
}
library(tm)
library(SnowballC)
library(dplyr)
pres_stats <- data.frame(
row.names = c("Democrat", "Barrack Obama", "William Clinton", "Republican", "George W. Bush", "Donald Trump"),
Words = c(53165, 26502, 26663, 53080, 26063, 27017))
knitr::kable(pres_stats,booktabs = T)
clean_corp <- function(corp) {
corp %>%
tm_map(tolower) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, c(stopwords("english"), "president", "applause", "will"))
}
Trump
J="a b c d e f g h i"
con_corp <- Corpus(DirSource("/Users/mukeshravichandran/Downloads/Mukesh_proj"), readerControl = list(language="lat"))
con_corp <- clean_corp(con_corp)
wordcloud(con_corp, random.order=FALSE, scale=c(3, .5), use.r.layout=TRUE,colors = brewer.pal(5, "Dark2"), max.words=75)
install.packages("wordcloud")
library(wordcloud)
wordcloud(con_corp, random.order=FALSE, scale=c(3, .5), use.r.layout=TRUE,colors = brewer.pal(5, "Dark2"), max.words=75)
con_tdm <- TermDocumentMatrix(con_corp)
con_m <- as.matrix(con_tdm)
con_df <- data.frame(word = row.names(con_m), con_m)
con_df$word <- as.character(con_df$word)
colnames(con_df) <- c("word", "count")
con_df %>% inner_join(get_sentiments("bing")) %>% group_by(sentiment) %>% summarize(sent_ttl = sum(count))
library(dplyr)
con_df %>% inner_join(get_sentiments("bing")) %>% group_by(sentiment) %>% summarize(sent_ttl = sum(count))
library(tidy)
install.packages("tidy")
install.packages("get_sentiments")
rm(list = ls())
library(haven)
library(forecast)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
library(zoo)
library(ggplot2)
library(lubridate)
library(gtable)
library(dplyr)
library(MLmetrics)
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Homework/#1/Data")
df=read.csv("Ozone_Raleigh2.csv")
View(df)
plot(df$Daily.Max.8.hour.Ozone.Concentration)
df=read.csv("Ozone_Raleigh2.csv",type='l')
df=read.csv("Ozone_Raleigh2.csv",type=l)
plot(df$Daily.Max.8.hour.Ozone.Concentration)
df=read.csv("Ozone_Raleigh2.csv",type='line')
df=read.csv("Ozone_Raleigh2.csv"
View(df)
plot(df$Daily.Max.8.hour.Ozone.Concentration,type='line'))
plot(df$Daily.Max.8.hour.Ozone.Concentration,type='line')
plot(df$Daily.Max.8.hour.Ozone.Concentration,type='b')
# convert character date into date
df$Date <- mdy(df$Date)
ggplot(data=df,aes(x=date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()
ggplot(data=df,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()
ggplot(data=df,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()+theme_classic()
ggplot(data=df,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_minimal()
ggplot(data=df,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()+theme_minimal()
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Homework/#1/Data")
df=read.csv("Ozone_Raleigh2.csv")
View(df)
asRules(tree)
#visualizing the tree
.pardefault = par()
# Rolling up the data to monthly #
df_q4<- aggregate( Daily.Max.8.hour.Ozone.Concentration ~ Month + Year , df , mean )
rm(list = ls())
library(haven)
library(forecast)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
library(zoo)
library(ggplot2)
library(lubridate)
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Time Series/Homework/#1/Data")
df=read.csv("Ozone_Raleigh2.csv")
rm(list = ls())
library(haven)
library(forecast)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
library(zoo)
library(ggplot2)
library(lubridate)
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Time Series/Homework/#1/Data")
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Homework/#1/Data")
df=read.csv("Ozone_Raleigh2.csv")
View(df)
# convert character date into date
df$Date <- mdy(df$Date)
# create new column for month and year #
df$Month <- month(df$Date)
df$Year <- year(df$Date)
# Rolling up the data to monthly #
df_q4<- aggregate( Daily.Max.8.hour.Ozone.Concentration ~ Month + Year , df , mean )
df_q4 = df_q4 %>% arrange(Year, Month)
df_q4$Date <- as.Date(as.yearmon(paste(df_q4$Year, df_q4$Month), "%Y %m"))
ggplot(df_q4, aes(x=Date, y=Daily.Max.8.hour.Ozone.Concentration)) +
geom_line()+scale_x_date(date_labels = "%Y")
View(df_q4)
#create time series object
ozone <- ts(df_q4$Daily.Max.8.hour.Ozone.Concentration, start = 2014, frequency =12)
# Time Series Decomposition ...STL#
decomp_stl <- stl(ozone, s.window = 7)
seasonal_stl<- decomp_stl$time.series[,1]
trend_stl <- decomp_stl$time.series[,2]
random_stl <- decomp_stl$time.series[,3]
# Create training set from overall Ozone Data
training=subset(ozone,end=length(ozone)-17)
# Create validation set from overall Ozone Data
valid=subset(ozone,start=length(ozone)-16, end=length(ozone)-5)
# Fit Holt-Winters ESM (multiplicative seasonality) on training data
HWES.ozone.train <- hw(training, seasonal = "multiplicative",initial='optimal')
summary(HWES.ozone.train)
# Forecast predictions from fitted HW ESM model on test data
valid.results1=forecast(HWES.ozone.train,h=12)
# Calculate prediction errors from forecast
error=valid-valid.results1$mean
# Calculate prediction error statistics (MAE and MAPE)
MAE=mean(abs(error))
MAPE=mean(abs(error)/abs(valid))
MAPE
nsdiffs(ozone)
ndiffs(diff(training,12))
diff(training,12)
plot(diff(training,12))
nuenet= nnetar(plot(diff(training,12)),p=2,P=1,size=2)
nuenet= nnetar(diff(training,12),p=2,P=1,size=2)
plot(forecast(nuenet,h=24))
library(haven)
library(forecast)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
library(zoo)
library(caschrono)
library(TSA)
library(quantmod)
# Change this for your computer
# This is the path for the folder where you saved/downloaded the SAS data sets
# for this course from the class webpage. The default folder name was 'Class Data'
file.dir <- "/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Class Data"
# Names of the data files from 'Class Data' that we're going to use
input.file1 <- "usairlines.sas7bdat"
# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines <- read_sas(paste(file.dir, input.file1,sep = ""))
# Change this for your computer
# This is the path for the folder where you saved/downloaded the SAS data sets
# for this course from the class webpage. The default folder name was 'Class Data'
file.dir <- "/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Class Data/"
# Names of the data files from 'Class Data' that we're going to use
input.file1 <- "usairlines.sas7bdat"
# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines <- read_sas(paste(file.dir, input.file1,sep = ""))
# Creating Time Series Data Objects
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency = 12)
# Autoregressive Neural Network Model and Forecast
NN.Model <- nnetar(diff(Passenger, 12), p = 2, P = 1, size = 2)
NN.Forecast <- forecast(NN.Model, h = 24)
plot(NN.Forecast)
# Neural Network Model on ARIMA residuals and forecast
xreg1<-cbind(fourier(Passenger,K=5),seq(1,length(Passenger)))
colnames(xreg1)<-c('s1','c1','s2','c2','s3','c3','s4','c4','s5','c5','time')
Model.four<-Arima(Passenger,order=c(0,0,0),xreg=xreg1)
NN.Model2<-nnetar(Model.four$residuals,p=2,P=1,size=2)
NN.Forecast2<-forecast(NN.Model2,h=24)
plot(NN.Forecast2)
# Forecast passengers for next year by adding to previous year's passenger data
Pass.Forecast <- rep(NA, 24)
for(i in 1:12){
Pass.Forecast[i] <- Passenger[length(Passenger) - 12 + i] + forecast(NN.Model, h = 24)$mean[i]
}
# Forecast passengers for next year by adding to the previously forecasted values
for(i in 13:24){
Pass.Forecast[i] <- Pass.Forecast[i - 12] + forecast(NN.Model, h = 24)$mean[i]
}
# Put the forecasted values into a time series object
Pass.Forecast <- ts(Pass.Forecast, start = c(2008, 4), frequency = 12)
# Plot the original (black) and our forecast (blue)
plot(Passenger, main = "US Airline Passengers ARIMA Model Forecasts", xlab = "Date", ylab = "Passengers (Thousands)", xlim = c(1990, 2010), ylim = c(30000,80000))
lines(Pass.Forecast, col = "blue")
abline(v = 2008.25, col = "red", lty = "dashed")
# Add the NN forecasted mean to a fourier time series forecast
for.seq<-seq(220,243)
xreg2<-cbind(fourier(Passenger,K=5,h=24),for.seq)
colnames(xreg2)<-c('s1','c1','s2','c2','s3','c3','s4','c4','s5','c5','time')
Base.forecast<-forecast(Model.four,xreg=xreg2,h=24)
Pass.Forecast2 <- Base.forecast$mean+NN.Forecast2$mean
# Put our second forecast into a time series object
Pass.Forecast2 <- ts(Pass.Forecast2, start = c(2008, 4), frequency = 12)
# Plot the original data (black) and our new forecast (orange)
plot(Passenger, main = "US Airline Passengers ARIMA Model Forecasts", xlab = "Date", ylab = "Passengers (Thousands)", xlim = c(1990, 2010), ylim = c(30000,80000))
lines(Pass.Forecast2, col = "orange")
abline(v = 2008.25, col = "red", lty = "dashed")
rm(list = ls())
temp = list.files(pattern="*.csv")
myfiles = lapply(temp, read.delim)
t
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Project/Data")
temp = list.files(pattern="*.csv")
myfiles = lapply(temp, read.delim)
View(myfiles)
myfiles[[1]]
view(myfiles[[1]])
View(myfiles[[1]])
temp = list.files(pattern="*.csv")
for (i in 1:length(temp)) assign(temp[i], read.csv(temp[i]))
View(CO_Raleigh.csv)
View(myfiles)
View(Ozone_Raleigh2.csv)
rm(list = ls())
library(haven)
library(forecast)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
library(zoo)
library(ggplot2)
library(lubridate)
library(gtable)
library(dplyr)
library(MLmetrics)
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Project/Data")
temp = list.files(pattern="*.csv")
for (i in 1:length(temp)) assign(temp[i], read.csv(temp[i]))
View(NO_Raleigh.csv)
View(Raleigh_weather.csv)
View(CO_Raleigh.csv)
View(CO_Raleigh.csv)
View(Ozone_Raleigh2.csv)
Ozone_Raleigh2.csv %>% full_join(CO_Raleigh.csv,by=Date)
J= Ozone_Raleigh2.csv %>% full_join(CO_Raleigh.csv,by=Date)
View(CO_Raleigh.csv)
View(Ozone_Raleigh2.csv)
J= Ozone_Raleigh2.csv %>% full_join(CO_Raleigh.csv,by="Date")
View(J)
J= Ozone_Raleigh2.csv %>% full_join(CO_Raleigh.csv,by="Date",suffix=NULL)
J= Ozone_Raleigh2.csv %>% full_join(CO_Raleigh.csv,by="Date",suffix=c("",""))
View(J)
Combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh,by="Date",suffix=c("","")))
Combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather.csv,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh,by="Date",suffix=c("","")))
View(CO_Raleigh.csv)
View(NO_Raleigh.csv)
View(Ozone_Raleigh2.csv)
View(Raleigh_weather.csv)
setnames(Raleigh_weather.csv, old = "DATE" , new = "Date")
library(dplyr)
setnames(Raleigh_weather.csv, old = "DATE" , new = "Date")
library(data.table)
setnames(Raleigh_weather.csv, old = "DATE" , new = "Date")
View(Raleigh_weather.csv)
Combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather.csv,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh,by="Date",suffix=c("","")))
Combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather.csv,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh.csv,by="Date",suffix=c("","")))
View(Combined)
Combined$Date <- mdy(Combined$Date)
rm(list = ls())
library(haven)
library(forecast)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
library(zoo)
library(ggplot2)
library(lubridate)
library(gtable)
library(dplyr)
library(MLmetrics)
library(data.table)
setwd("/Users/mukeshravichandran/OneDrive - North Carolina State University/MSA-Mukesh’s MacBook Pro/Courses/Fall/AA502/Fall 1/Time Series/Project/Data")
temp = list.files(pattern="*.csv")
for (i in 1:length(temp)) assign(temp[i], read.csv(temp[i], as.dat))
setnames(Raleigh_weather.csv, old = "DATE" , new = "Date")
temp = list.files(pattern="*.csv")
for (i in 1:length(temp)) assign(temp[i], read.csv(temp[i], as.dat))
for (i in 1:length(temp)) assign(temp[i], read.csv(temp[i]))
setnames(Raleigh_weather.csv, old = "DATE" , new = "Date")
combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather.csv,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh.csv,by="Date",suffix=c("","")))
combined$Date <- mdy(combined$Date)
View(combined)
combined =combined[order(Date)]
combined =combined[order("Date")]
View(combined)
combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather.csv,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh.csv,by="Date",suffix=c("","")))
combined$Date <- mdy(combined$Date)
combined =combined[order(Date),]
combined =combined[order("Date"),]
View(combined)
library(haven)
library(forecast)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather.csv,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh.csv,by="Date",suffix=c("","")))
combined$Date <- mdy(combined$Date)
combined =combined[order(Date),]
combined = (Ozone_Raleigh2.csv %>%
full_join(CO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(NO_Raleigh.csv,by="Date",suffix=c("","")) %>%
full_join(Raleigh_weather.csv,by="Date",suffix=c("","")) %>%
full_join(SO2_Raleigh.csv,by="Date",suffix=c("","")))
combined$Date <- mdy(combined$Date)
combined =combined[order(combined$Date),]
View(combined)
combined =combined[order(combined$Date),]
View(combined)
colnames(combined)
ggplot(data=combined,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()
ggplot(data=combined,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()+grom_line(aes(x=Date,y=SNOW))
ggplot(data=combined,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()+geom_line(aes(x=Date,y=SNOW))
View(SO2_Raleigh.csv)
ggplot(data=combined,aes(x=Date,y=Daily.Max.8.hour.Ozone.Concentration))+geom_line()+geom_line(aes(x=Date,y=Daily.Max.1.hour.NO2.Concentration))
ggplot(combined, aes(x=Date)) +
geom_line(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred") +
geom_line(aes(y = Daily.Max.1.hour.NO2.Concentration), color="steelblue", linetype="twodash")
ggplot(combined, aes(x=Date)) +
geom_line(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred") +
geom_line(aes(y = Daily.Max.1.hour.NO2.Concentration), color="steelblue", linetype="twodash")+scale_color_manual(values = c("darkred", "steelblue"))
geom_line(aes(y = Daily.Max.1.hour.NO2.Concentration) + scale_y_continuous(trans='log10')
ggplot(combined, aes(x=Date)) +
geom_line(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred") +
geom_line(aes(y = Daily.Max.1.hour.NO2.Concentration)) + scale_y_continuous(trans='log10')
ggplot(combined, aes(x=Date)) +
geom_line(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred") +
geom_line(aes(y = Daily.Max.1.hour.NO2.Concentration)) + scale_y_continuous(trans='log10') +
geom_line(aes(y = Daily.Max.1.hour.SO2.Concentration))
ggplot(combined, aes(x=Date)) +
geom_point(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred") +
geom_point(aes(y = Daily.Max.1.hour.NO2.Concentration)) + scale_y_continuous(trans='log10') +
geom_point(aes(y = Daily.Max.1.hour.SO2.Concentration))
sapply(combined,na.interpolation,option-"spline")
library(stinepack)
library(stinepack)
detach("package:stinepack", unload = TRUE)
library(stinepack)
sapply(combined,na.interpolation,option-"spline")
sapply(combined,na.interpolation,option-"spline")
library(stinepack)
sapply(combined,na_interpolation,option="spline",maxgap=5)
stinepack::na_interpolation
library(imputeTS)
sapply(combined,na_interpolation,option="spline",maxgap=5)
sapply(combined[-Date],na_interpolation,option="spline",maxgap=5)
isNumeric(combined[1])
library(limma)
install.packages("limma")
isNumeric(Combined)
isNumeric(combined)
ggplot(combined, aes(x=Date)) +
geom_point(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred") +
geom_point(aes(y = Daily.Max.1.hour.NO2.Concentration)) + scale_y_continuous(trans='log10') +
geom_point(aes(y = Daily.Max.1.hour.SO2.Concentration))
na.interpolation
na.interpolation(combined$Daily.Max.8.hour.Ozone.Concentration,option = "spline", maxgap = 5)
View(combined)
na.interpolation(combined$Daily.Max.8.hour.Ozone.Concentration,option = "spline", maxgap = 10)
na_interpolation(combined$Daily.Max.8.hour.Ozone.Concentration,option = "spline", maxgap = 10)
ggplot(combined, aes(x=Date)) +
geom_point(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred")
ggplot(combined, aes(x=Date)) +
geom_line(aes(y = Daily.Max.8.hour.Ozone.Concentration), color = "darkred")
ggplot(combined, aes(x=Date)) +
geom_line(aes(y = Daily.Max.8.hour.Ozone.Concentration))
combined$Date <- mdy(combined$Date)
